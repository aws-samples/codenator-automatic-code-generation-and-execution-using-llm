{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e19b3097-4f56-4bf3-97c3-f664bf1065fd",
   "metadata": {},
   "source": [
    "# Prepare dependency packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ac3a7fb-7c80-4177-ae53-2308bfc2819e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.29.42 requires botocore==1.31.42, but you have botocore 1.31.63 which is incompatible.\n",
      "awscli 1.29.42 requires s3transfer<0.7.0,>=0.6.0, but you have s3transfer 0.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install jsonpath_ng boto3 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e46c2be-0d28-42cb-b4e8-125aa9f34939",
   "metadata": {},
   "source": [
    "# Prompt examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffd7b3b2-e392-4d0f-9d46-5d597cc64570",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params_examples = [\n",
    "    {\n",
    "        \"prompt\": \"Human: Hello there!\\\\nAssistant:\",\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"repetition_penalty\": 1.0,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.3,\n",
    "        \"top_k\": 1,\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Human: Hello there!\\\\nAssistant:\",\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"repetition_penalty\": 1.0,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.3,\n",
    "        \"top_k\": 1,\n",
    "        \"stream\": True\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Hello there!\",\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"repetition_penalty\": 1.0,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.3,\n",
    "        \"top_k\": 1,\n",
    "        \"stop\": [\"\\nUser\", \"endoftext\"]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3f844d-c6e1-4cd7-935e-047626c8a92e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7d09ff-8338-4933-99db-8aadd5fdb7c0",
   "metadata": {},
   "source": [
    "## Available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "713a45e7-59ee-4093-942c-39f59e8281e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon.titan-tg1-large\n",
      "amazon.titan-e1t-medium\n",
      "amazon.titan-embed-g1-text-02\n",
      "amazon.titan-text-express-v1\n",
      "amazon.titan-embed-text-v1\n",
      "stability.stable-diffusion-xl\n",
      "stability.stable-diffusion-xl-v0\n",
      "ai21.j2-grande-instruct\n",
      "ai21.j2-jumbo-instruct\n",
      "ai21.j2-mid\n",
      "ai21.j2-mid-v1\n",
      "ai21.j2-ultra\n",
      "ai21.j2-ultra-v1\n",
      "anthropic.claude-instant-v1\n",
      "anthropic.claude-v1\n",
      "anthropic.claude-v2\n",
      "cohere.command-text-v14\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "br = boto3.client(\n",
    "    \"bedrock\",\n",
    "    region_name=\"us-west-2\"\n",
    ")\n",
    "\n",
    "for model in br.list_foundation_models()[\"modelSummaries\"]:\n",
    "    print(model[\"modelId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae121f89-2d80-453e-9878-deb758cbddc1",
   "metadata": {},
   "source": [
    "## Invoke without streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52f82fdb-f5fa-4fce-8e40-44e6ea07c757",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: amazon.titan-tg1-large - TEXT:  Hello, how can I help you today?\n",
      "model: ai21.j2-grande-instruct - TEXT:  How can I assist you today?\n",
      "I'm here to help you with your questions.\n",
      "model: ai21.j2-jumbo-instruct - TEXT:  How can I assist you today?\n",
      "I'm here to help you with any questions you may have. How can I assist you today?\n",
      "model: ai21.j2-mid - TEXT:  How can I assist you today?\n",
      "I'm here to help you with your questions.\n",
      "model: ai21.j2-mid-v1 - TEXT:  How can I assist you today?\n",
      "I'm here to help you with your questions.\n",
      "model: ai21.j2-ultra - TEXT:  How can I assist you today?\n",
      "I'm here to help you with any questions you may have. How can I assist you today?\n",
      "model: ai21.j2-ultra-v1 - TEXT:  How can I assist you today?\n",
      "I'm here to help you with any questions you may have. How can I assist you today?\n",
      "model: anthropic.claude-instant-v1 - TEXT:  Hello!\n",
      "model: anthropic.claude-v1 - TEXT:  Hello! My name is Claude.\n",
      "model: anthropic.claude-v2 - TEXT:  Hello! Nice to meet you.\n",
      "model: cohere.command-text-v14 - TEXT:  Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from importlib import import_module\n",
    "\n",
    "model_family = \"bedrock\"\n",
    "model_names = [\n",
    "    \"amazon.titan-tg1-large\",\n",
    "    \"ai21.j2-grande-instruct\",\n",
    "    \"ai21.j2-jumbo-instruct\",\n",
    "    \"ai21.j2-mid\",\n",
    "    \"ai21.j2-mid-v1\",\n",
    "    \"ai21.j2-ultra\",\n",
    "    \"ai21.j2-ultra-v1\",\n",
    "    \"anthropic.claude-instant-v1\",\n",
    "    \"anthropic.claude-v1\",\n",
    "    \"anthropic.claude-v2\",\n",
    "    \"cohere.command-text-v14\"\n",
    "]\n",
    "\n",
    "for model_name in model_names:\n",
    "    invoke = import_module(\"handlers.\" + model_family).model(model_name).invoke\n",
    "    print(f\"model: {model_name} - TEXT: {invoke(params_examples[0])['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ec5851-c4f0-4f75-97fa-4fbebc28a88a",
   "metadata": {},
   "source": [
    "## Invoke with streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb521d7f-4f0a-42f8-b3e9-3d9354d6c0be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model: anthropic.claude-instant-v1 - TEXT:  Hello!\n",
      "model: anthropic.claude-v1 - TEXT:  Hello! My name is Claude.\n",
      "model: anthropic.claude-v2 - TEXT:  Hello! Nice to meet you.\n",
      "model: cohere.command-text-v14 - TEXT:  Hello! How can I assist you today?"
     ]
    }
   ],
   "source": [
    "from importlib import import_module\n",
    "\n",
    "model_family = \"bedrock\"\n",
    "model_names = [\n",
    "    # \"amazon.titan-tg1-large\",\n",
    "    # \"ai21.j2-grande-instruct\",\n",
    "    # \"ai21.j2-jumbo-instruct\",\n",
    "    # \"ai21.j2-mid\",\n",
    "    # \"ai21.j2-mid-v1\",\n",
    "    # \"ai21.j2-ultra\",\n",
    "    # \"ai21.j2-ultra-v1\",\n",
    "    \"anthropic.claude-instant-v1\",\n",
    "    \"anthropic.claude-v1\",\n",
    "    \"anthropic.claude-v2\",\n",
    "    \"cohere.command-text-v14\"\n",
    "]\n",
    "    \n",
    "for model_name in model_names:\n",
    "    invoke = import_module(\"handlers.\" + model_family).model(model_name).invoke_with_response_stream\n",
    "    print()\n",
    "    print(f\"model: {model_name} - TEXT: \", end=\"\")\n",
    "    for i in invoke(params_examples[1]):\n",
    "        if \"generated_text\"  in i and i[\"generated_text\"] != \"<EOS_TOKEN>\":\n",
    "            print(i['generated_text'], end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c970af7d-c5de-443f-bd8d-d007f7aaf47a",
   "metadata": {},
   "source": [
    "# SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd671815-5e46-46fb-a542-cf7f0293b0bf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Deploy TGI endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f751fb22-2eaf-47ea-bc3b-a9ce07074308",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  version=\"0.8.2\"\n",
    ")\n",
    "\n",
    "role = get_execution_role()\n",
    "hf_model_id = \"tiiuae/falcon-7b-instruct\" # model id from huggingface.co/models\n",
    "model_name = hf_model_id.replace(\"/\",\"-\").replace(\".\",\"-\")\n",
    "endpoint_name = \"test-sagemaker-02\"\n",
    "instance_type = \"ml.g5.2xlarge\" # instance type to use for deployment\n",
    "number_of_gpus = 1 # number of gpus to use for inference and tensor parallelism\n",
    "health_check_timeout = 900 # Increase the timeout for the health check to 5 minutes for downloading the model\n",
    "\n",
    "llm_model = HuggingFaceModel(\n",
    "      role=role,\n",
    "      image_uri=llm_image,\n",
    "      env={\n",
    "        'HF_MODEL_ID': hf_model_id,\n",
    "        # 'HF_MODEL_QUANTIZE': \"bitsandbytes\", # comment in to quantize\n",
    "        'SM_NUM_GPUS': f\"{number_of_gpus}\",\n",
    "        'MAX_INPUT_LENGTH': \"3000\",  # Max length of input text\n",
    "        'MAX_TOTAL_TOKENS': \"6000\",  # Max length of the generation (including input text)\n",
    "        'HF_MODEL_REVISION': 'eb410fb6ffa9028e97adb801f0d6ec46d02f8b07'\n",
    "      },\n",
    "      name=model_name\n",
    "    )\n",
    "\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout,\n",
    "  endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3857831-de8a-45d2-8fde-a57fa92b9e5f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Invoke with no stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09b22866-5cd5-47b0-84f5-7e0a14fbc9a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: tgi.test-sagemaker-02 - TEXT:  Hi there! How can I help you today?\n",
      "User \n"
     ]
    }
   ],
   "source": [
    "from importlib import import_module\n",
    "\n",
    "model_family = \"sagemaker\"\n",
    "model_name = \"tgi.test-sagemaker-02\"\n",
    "\n",
    "invoke = import_module(\"handlers.\" + model_family).model(model_name).invoke\n",
    "print(f\"model: {model_name} - TEXT: {invoke(params_examples[0])['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305aec1c-abbe-4e03-b806-483beafe6476",
   "metadata": {},
   "source": [
    "## Invoke with stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4880c75-13ae-4fbd-89e8-4b2eb46f6c7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model: tgi.test-sagemaker-02 - TEXT:  Hi Hi there there!! How How can can I I help help you you today today??\n",
      "\n",
      "UserUser  <|endoftext|><|endoftext|>"
     ]
    }
   ],
   "source": [
    "from importlib import import_module\n",
    "\n",
    "model_family = \"sagemaker\"\n",
    "model_name = \"tgi.test-sagemaker-02\"\n",
    "\n",
    "invoke = import_module(\"handlers.\" + model_family).model(model_name).invoke_with_response_stream\n",
    "print()\n",
    "print(f\"model: {model_name} - TEXT: \", end=\"\")\n",
    "for i in invoke(params_examples[1]):\n",
    "    if \"generated_text\"  in i and i[\"generated_text\"] != \"<EOS_TOKEN>\":\n",
    "        print(i['generated_text'], end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c86c7a6-57ea-4543-8fa3-af50bb5b1e99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sha256:e2a26379348c5f0dfee0dbefeb5b9ecdd2f62b776ee365df919857b9ddc8f0c4\n"
     ]
    }
   ],
   "source": [
    "!docker build --quiet --tag api-layer:latest ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ef49cdd-5479-4e64-9b40-b0f15a50c4e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1edb3cffa4553126b329267b132549f892904f0e6c835fc6da245cb2e4dbe1a1\n"
     ]
    }
   ],
   "source": [
    "!docker run --name test-api-layer -d -p 8001:8001 api-layer:latest --host 0.0.0.0 --port 8001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "97eb5403-5d49-4642-a0d6-891976905bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-api-layer\n",
      "test-api-layer\n"
     ]
    }
   ],
   "source": [
    "!docker kill test-api-layer && docker rm test-api-layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "37803dfd-b682-433b-8600-16597bf924bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-api-layer\n"
     ]
    }
   ],
   "source": [
    "!docker rm test-api-layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9737423e-d614-4acd-973a-5e93a95d33a1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [1]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)\n",
      "INFO:     172.31.68.47:53078 - \"POST /invoke HTTP/1.1\" 200 OK\n",
      "INFO:     172.31.68.47:59536 - \"POST /invoke_stream HTTP/1.1\" 200 OK\n",
      "INFO:     172.31.68.47:45778 - \"POST /invoke_stream HTTP/1.1\" 500 Internal Server Error\n",
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/site-packages/uvicorn/protocols/http/h11_impl.py\", line 408, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"/usr/local/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/fastapi/applications.py\", line 292, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/applications.py\", line 122, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 184, in __call__\n",
      "    raise exc\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 162, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 79, in __call__\n",
      "    raise exc\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 68, in __call__\n",
      "    await self.app(scope, receive, sender)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\", line 20, in __call__\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\", line 17, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/routing.py\", line 718, in __call__\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/routing.py\", line 276, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/routing.py\", line 66, in app\n",
      "    response = await func(request)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/fastapi/routing.py\", line 273, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/fastapi/routing.py\", line 190, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "  File \"/code/app/main.py\", line 40, in invoke_stream\n",
      "    params = await request.json()\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/requests.py\", line 244, in json\n",
      "    self._json = json.loads(body)\n",
      "  File \"/usr/local/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/usr/local/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/usr/local/lib/python3.10/json/decoder.py\", line 355, in raw_decode\n",
      "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
      "json.decoder.JSONDecodeError: Expecting value: line 1 column 12 (char 11)\n",
      "INFO:     172.31.68.47:37586 - \"POST /invoke_stream HTTP/1.1\" 500 Internal Server Error\n",
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/site-packages/uvicorn/protocols/http/h11_impl.py\", line 408, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"/usr/local/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/fastapi/applications.py\", line 292, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/applications.py\", line 122, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 184, in __call__\n",
      "    raise exc\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 162, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 79, in __call__\n",
      "    raise exc\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 68, in __call__\n",
      "    await self.app(scope, receive, sender)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\", line 20, in __call__\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\", line 17, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/routing.py\", line 718, in __call__\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/routing.py\", line 276, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/routing.py\", line 66, in app\n",
      "    response = await func(request)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/fastapi/routing.py\", line 273, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/fastapi/routing.py\", line 190, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "  File \"/code/app/main.py\", line 40, in invoke_stream\n",
      "    params = await request.json()\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/requests.py\", line 244, in json\n",
      "    self._json = json.loads(body)\n",
      "  File \"/usr/local/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/usr/local/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/usr/local/lib/python3.10/json/decoder.py\", line 355, in raw_decode\n",
      "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
      "json.decoder.JSONDecodeError: Expecting value: line 1 column 12 (char 11)\n",
      "INFO:     172.31.68.47:34254 - \"POST /invoke_stream HTTP/1.1\" 500 Internal Server Error\n",
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/site-packages/uvicorn/protocols/http/h11_impl.py\", line 408, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"/usr/local/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/fastapi/applications.py\", line 292, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/applications.py\", line 122, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 184, in __call__\n",
      "    raise exc\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 162, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 79, in __call__\n",
      "    raise exc\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 68, in __call__\n",
      "    await self.app(scope, receive, sender)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\", line 20, in __call__\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\", line 17, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/routing.py\", line 718, in __call__\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/routing.py\", line 276, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/routing.py\", line 66, in app\n",
      "    response = await func(request)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/fastapi/routing.py\", line 273, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/fastapi/routing.py\", line 190, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "  File \"/code/app/main.py\", line 40, in invoke_stream\n",
      "    params = await request.json()\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/requests.py\", line 244, in json\n",
      "    self._json = json.loads(body)\n",
      "  File \"/usr/local/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/usr/local/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/usr/local/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid control character at: line 1 column 51 (char 50)\n",
      "INFO:     172.31.68.47:57452 - \"POST /invoke_stream HTTP/1.1\" 500 Internal Server Error\n",
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/site-packages/uvicorn/protocols/http/h11_impl.py\", line 408, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"/usr/local/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/fastapi/applications.py\", line 292, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/applications.py\", line 122, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 184, in __call__\n",
      "    raise exc\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 162, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 79, in __call__\n",
      "    raise exc\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 68, in __call__\n",
      "    await self.app(scope, receive, sender)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\", line 20, in __call__\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\", line 17, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/routing.py\", line 718, in __call__\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/routing.py\", line 276, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/routing.py\", line 66, in app\n",
      "    response = await func(request)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/fastapi/routing.py\", line 273, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/fastapi/routing.py\", line 190, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "  File \"/code/app/main.py\", line 40, in invoke_stream\n",
      "    params = await request.json()\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/requests.py\", line 244, in json\n",
      "    self._json = json.loads(body)\n",
      "  File \"/usr/local/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/usr/local/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/usr/local/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Invalid control character at: line 1 column 51 (char 50)\n",
      "INFO:     172.31.68.47:56244 - \"POST /invoke_stream HTTP/1.1\" 500 Internal Server Error\n",
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/site-packages/uvicorn/protocols/http/h11_impl.py\", line 408, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"/usr/local/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/fastapi/applications.py\", line 292, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/applications.py\", line 122, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 184, in __call__\n",
      "    raise exc\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 162, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 79, in __call__\n",
      "    raise exc\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 68, in __call__\n",
      "    await self.app(scope, receive, sender)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\", line 20, in __call__\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\", line 17, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/routing.py\", line 718, in __call__\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/routing.py\", line 276, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/starlette/routing.py\", line 66, in app\n",
      "    response = await func(request)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/fastapi/routing.py\", line 273, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/fastapi/routing.py\", line 190, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "  File \"/code/app/main.py\", line 45, in invoke_stream\n",
      "    raise\n",
      "RuntimeError: No active exception to reraise\n"
     ]
    }
   ],
   "source": [
    "!docker logs test-api-layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8552cf70-01ca-421f-b342-1c3a1fc7cab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                              DESCRIPTION                               DOCKER ENDPOINT                                               ERROR\n",
      "default                           Current DOCKER_HOST based configuration   unix:///var/run/docker.sock                                   \n",
      "m5.xlarge_i-0c2d8f6ba19eeb809 *                                             tcp://ip-172-31-70-124.ap-southeast-2.compute.internal:1111   \n"
     ]
    }
   ],
   "source": [
    "!docker context ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38484312-a5b4-4fe1-a260-9a6fda377762",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "host_ip = \"ip-172-31-74-201.ap-southeast-2.compute.internal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f57d539f-57e9-4556-beb1-10784695ed7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   868  100   714  100   154     53     11  0:00:14  0:00:13  0:00:01   177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"generated_text\":\" Here is a poem about a young girl named Zee:\\n\\nZee, the girl with eyes so bright\\nFull of spirit, full of light\\nHair flowing free in the breeze\\nLaughing, playing with ease\\nAdventurous, fun and free\\nFilled with joy for all to see\\nOn the swings, she loves to fly\\nReaching up to touch the sky  \\nWith imagination unbound\\nNew worlds and stories can be found\\nIn her mind, where fairies dwell\\nAnd magic weaves a happy spell\\nZee lives each day with wonder and glee\\nThe world's a playground, just wait and see\\nA smile, a laugh, she spreads them around\\nTo all she meets, joy does abound\\nHer spirit shines for all to see\\nZee, a girl full of life's beauty\",\"finish_reason\":\"stop_sequence\"}"
     ]
    }
   ],
   "source": [
    "%%bash -s $host_ip\n",
    "curl -X POST ${1}:8001/invoke -d '{\"body\":{\"prompt\": \"Human: Hello, write a poem about a young girl named Zee\\nAssistant:\"}, \"model_family\": \"bedrock\", \"model_name\": \"anthropic.claude-v2\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cb7decb-cb7c-49a5-99b2-7b025ae6d493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  2008    0  1854  100   154     76      6  0:00:25  0:00:24  0:00:01   260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"generated_text\": \" Here\", \"finish_reason\": null}\n",
      "{\"generated_text\": \" is a poem about a\", \"finish_reason\": null}\n",
      "{\"generated_text\": \" young girl named Zee\", \"finish_reason\": null}\n",
      "{\"generated_text\": \":\\n\\nZee,\", \"finish_reason\": null}\n",
      "{\"generated_text\": \" the\", \"finish_reason\": null}\n",
      "{\"generated_text\": \" girl with\", \"finish_reason\": null}\n",
      "{\"generated_text\": \" eyes so\", \"finish_reason\": null}\n",
      "{\"generated_text\": \" bright\", \"finish_reason\": null}\n",
      "{\"generated_text\": \"\\nFull\", \"finish_reason\": null}\n",
      "{\"generated_text\": \" of spirit, full of\", \"finish_reason\": null}\n",
      "{\"generated_text\": \" light\\nHair flowing free in\", \"finish_reason\": null}\n",
      "{\"generated_text\": \" the breeze\\nLaughing\", \"finish_reason\": null}\n",
      "{\"generated_text\": \", playing with ease\", \"finish_reason\": null}\n",
      "{\"generated_text\": \"\\nAdventurous, fun\", \"finish_reason\": null}\n",
      "{\"generated_text\": \" and free\\nFilled\", \"finish_reason\": null}\n",
      "{\"generated_text\": \" with joy for all to see\", \"finish_reason\": null}\n",
      "{\"generated_text\": \"\\nOn the swings\", \"finish_reason\": null}\n",
      "{\"generated_text\": \", she loves to fly\\nRe\", \"finish_reason\": null}\n",
      "{\"generated_text\": \"aching up to touch\", \"finish_reason\": null}\n",
      "{\"generated_text\": \" the sky  \\nWith imagination\", \"finish_reason\": null}\n",
      "{\"generated_text\": \" unbound\\nNew worlds\", \"finish_reason\": null}\n",
      "{\"generated_text\": \" and stories can be\", \"finish_reason\": null}\n",
      "{\"generated_text\": \" found\\nIn her mind, where f\", \"finish_reason\": null}\n",
      "{\"generated_text\": \"airies dwell\\nAnd magic happens\", \"finish_reason\": null}\n",
      "{\"generated_text\": \" with each spell\\nZee lives each\", \"finish_reason\": null}\n",
      "{\"generated_text\": \" day filled with delight  \", \"finish_reason\": null}\n",
      "{\"generated_text\": \"\\nA shining girl\", \"finish_reason\": null}\n",
      "{\"generated_text\": \" with eyes so\", \"finish_reason\": null}\n",
      "{\"generated_text\": \" bright\", \"finish_reason\": \"stop_sequence\"}\n"
     ]
    }
   ],
   "source": [
    "%%bash -s $host_ip\n",
    "curl -X POST ${1}:8001/invoke_stream -d '{\"body\":{\"prompt\": \"Human: Hello, write a poem about a young girl named Zee\\nAssistant:\"}, \"model_family\": \"bedrock\", \"model_name\": \"anthropic.claude-v2\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9e5a04-7d1d-4c60-af88-4ed2fe417c09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d8c6092-dd8b-4880-9d94-26a6ba052e29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CODE_INTERPRETER_SYSTEM_PROMPT = \"\"\"You are a helpful AI assistant.\n",
    "\n",
    "You have access to a python code interpreter, which supports you in your tasks.\n",
    "The code is executed in an interactive shell, imports and variables are preserved between calls.\n",
    "The environment has internet and file system access.\n",
    "The current working directory is shared with the user, so files can be exchanged.\n",
    "There are many libraries pre-installed, including numpy, pandas, matplotlib, and scikit-learn.\n",
    "You cannot show rich outputs like plots or images, but you can store them in the working directory and point the user to them.\n",
    "If the code runs too long, there will be a timeout.\n",
    "\n",
    "To access the interpreter, use the following format:\n",
    "```python\n",
    "<your code>\n",
    "```\n",
    "If you want to call Python and still say something, do only output text above the code block, NOT below.\n",
    "Only provide at most one code block per message.\n",
    "The code will be executed automatically and the result will be sent back to you\n",
    "\"\"\"\n",
    "\n",
    "ROLES = [\"Human\", \"Assistant\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc5cab20-e1fb-40e4-beb9-9757ee8273c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e49777b-c3d4-4db8-86f1-889eab8e8203",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jsonpath_ng'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat is 52th fibonacci number?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m prompt \u001b[38;5;241m=\u001b[39m ROLES[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m CODE_INTERPRETER_SYSTEM_PROMPT \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m QUESTION: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m text \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/n\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m ROLES[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 10\u001b[0m invoke \u001b[38;5;241m=\u001b[39m \u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhandlers.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_family\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmodel(model_name)\u001b[38;5;241m.\u001b[39minvoke_with_response_stream\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - TEXT: \u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/Projects/synthcodex-automatic-code-generation-and-execution-using-llm/api_layer/app/handlers/bedrock.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mboto3\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhandlers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlogger\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mStreamIterator\u001b[39;00m:\n",
      "File \u001b[0;32m~/Projects/synthcodex-automatic-code-generation-and-execution-using-llm/api_layer/app/handlers/base.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjsonpath_ng\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m jsonpath, parse\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBaseModel\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'jsonpath_ng'"
     ]
    }
   ],
   "source": [
    "from importlib import import_module\n",
    "\n",
    "model_family = \"bedrock\"\n",
    "model_name = \"anthropic.claude-v2\"\n",
    "\n",
    "text = \"what is 52th fibonacci number?\"\n",
    "\n",
    "prompt = ROLES[0] + \": \" + CODE_INTERPRETER_SYSTEM_PROMPT + \" QUESTION: \" + text + \"/n\" + ROLES[1] + \":\"\n",
    "\n",
    "invoke = import_module(\"handlers.\" + model_family).model(model_name).invoke_with_response_stream\n",
    "print()\n",
    "print(f\"model: {model_name} - TEXT: \", end=\"\")\n",
    "for i in invoke({\"prompt\": prompt}):\n",
    "    if \"generated_text\"  in i and i[\"generated_text\"] != \"<EOS_TOKEN>\":\n",
    "        print(i['generated_text'], end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f2b616e-1c5a-4912-942f-d653cdffd634",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = '{\"body\": {\"prompt\": \"' + prompt +'\"}, \"model_family\": \"' + model_family + '\", \"model_name\": \"' + model_name + '\"}'\n",
    "data = data.replace(\"\\n\", \"\\\\n\")\n",
    "# print(data.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e491af6-1567-45af-8dac-5c84d5140192",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'body': {'prompt': 'Human: You are a helpful AI assistant.\\n\\nYou have access to a python code interpreter, which supports you in your tasks.\\nThe code is executed in an interactive shell, imports and variables are preserved between calls.\\nThe environment has internet and file system access.\\nThe current working directory is shared with the user, so files can be exchanged.\\nThere are many libraries pre-installed, including numpy, pandas, matplotlib, and scikit-learn.\\nYou cannot show rich outputs like plots or images, but you can store them in the working directory and point the user to them.\\nIf the code runs too long, there will be a timeout.\\n\\nTo access the interpreter, use the following format:\\n```python\\n<your code>\\n```\\nIf you want to call Python and still say something, do only output text above the code block, NOT below.\\nOnly provide at most one code block per message.\\nThe code will be executed automatically and the result will be sent back to you\\n QUESTION: what is 52th fibonacci number?/nAssistant:'},\n",
       " 'model_family': 'bedrock',\n",
       " 'model_name': 'anthropic.claude-v2'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3c29dea-d99f-4aa0-b1ec-c41cc048e0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here is the code to get the 52nd Fibonacci number:\n",
      "\n",
      "```python\n",
      "a, b = 0, 1\n",
      "for i in range(50):\n",
      "    a, b = b, a + b\n",
      "print(a)\n",
      "```\n",
      "\n",
      "The 52nd Fibonacci number is: 806515533049393"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "host_url = f\"http://{host_ip}:8001/invoke_stream\"\n",
    "\n",
    "def iter_func(result):\n",
    "        for chunk in result.iter_lines():\n",
    "            yield json.loads(chunk)[\"generated_text\"]\n",
    "\n",
    "res = requests.post(\n",
    "    url=host_url,\n",
    "    data=data,\n",
    "    stream=True\n",
    ")\n",
    "for chunk in iter_func(res):\n",
    "    print(chunk, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f1732aa8-58fe-4acb-9ad8-d05bfb836d8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "api_layer_url = f\"http://{host_ip}:8001/invoke\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "78184728-79cb-4a75-ad28-e0c17314ce2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://ip-172-31-75-227.ap-southeast-2.compute.internal:8001/invoke_stream\n",
      "http://ip-172-31-75-227.ap-southeast-2.compute.internal:8001/invoke_stream\n"
     ]
    }
   ],
   "source": [
    "stream = True\n",
    "print(host_url)\n",
    "print(api_layer_url + (\"\" if not stream else \"_stream\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "39168ea7-4aaa-45d6-bba2-460299e94b53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def send_req_to_agent(text, model_family, model_name, stream=False):\n",
    "    def iter_func(res):\n",
    "        for chunk in res.iter_lines():\n",
    "            # chunck_dict = json.loads(chunk)\n",
    "            # yield json.loads(chunk)[\"generated_text\"]\n",
    "            chunck_dict = json.loads(chunk)\n",
    "            yield chunck_dict[\"generated_text\"]\n",
    "        yield \"|STOP|\"\n",
    "    \n",
    "    data = {\n",
    "        \"body\": {\n",
    "            \"prompt\": text\n",
    "        }, \n",
    "        \"model_family\": model_family, \n",
    "        \"model_name\": model_name\n",
    "    }\n",
    "    ret = requests.post(\n",
    "        url=api_layer_url + (\"\" if not stream else \"_stream\"), \n",
    "        data=json.dumps(data),\n",
    "        stream=stream\n",
    "    )\n",
    "    if stream:\n",
    "        return iter_func(ret)\n",
    "    else:\n",
    "        return json.loads(ret.text)[\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "906946e2-ed7f-40f3-b709-3a0b0a8dd87c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Here is the code to get the 52nd Fibonacci number:\\n\\n```python\\na, b = 0, 1\\nfor i in range(50):\\n    a, b = b, a + b\\nprint(a)\\n```\\n\\nThe 52nd Fibonacci number is: 806515533049393'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "send_req_to_agent(prompt, model_family, model_name, stream=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2e58aa92-87fe-4046-9307-b16a2f9575fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here is the code to get the 52nd Fibonacci number:\n",
      "\n",
      "```python\n",
      "a, b = 0, 1\n",
      "for i in range(50):\n",
      "    a, b = b, a + b\n",
      "print(a)\n",
      "```\n",
      "\n",
      "The 52nd Fibonacci number is: 806515533049393\n",
      "\n",
      " Stream ended\n"
     ]
    }
   ],
   "source": [
    "for text in send_req_to_agent(prompt, model_family, model_name, stream=True):\n",
    "    # print(text, end=\"\")\n",
    "    if not text == \"|STOP|\":\n",
    "        print(text, end=\"\")\n",
    "    else:\n",
    "        print(\"\\n\\n Stream ended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39822f46-e8d2-4741-8d3b-062ee90e53a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-2:452832661640:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "lcc_arn": "arn:aws:sagemaker:ap-southeast-2:734840029783:studio-lifecycle-config/sdocker-kg"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
