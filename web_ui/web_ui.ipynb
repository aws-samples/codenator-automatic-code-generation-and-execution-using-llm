{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "8ad0e68f-8280-444c-a2d0-a61b451b6fe2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install gradio -U -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa74c53-1bff-4be7-b14a-fc0628bfc010",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n",
      "Sagemaker notebooks may require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Running on public URL: https://c2af54fe5457e0fe1a.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://c2af54fe5457e0fe1a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "import gradio as gr\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Global\n",
    "controller_url = \"ip-172-31-77-9.ap-southeast-2.compute.internal\" + \":8000\"\n",
    "\n",
    "l_mapping = {\n",
    "    \"Bash\":\"shell\", \n",
    "    \"Python\":\"python\", \n",
    "    \"Java\":\"javascript\",\n",
    "    \"JavaScript\":\"javascript\", \n",
    "    \"R\":\"r\"\n",
    "}\n",
    "models_list = {\n",
    "    \"Claude-v2\": {\n",
    "        \"model_family\": \"bedrock\",\n",
    "        \"model_name\": \"anthropic.claude-v2\"\n",
    "    },\n",
    "    \"Claude-v1\": {\n",
    "        \"model_family\": \"bedrock\",\n",
    "        \"model_name\": \"anthropic.claude-v1\"\n",
    "    },\n",
    "}\n",
    "\n",
    "out_tag = [\"<output>\", \"/output>\"]\n",
    "ex_out_tag = [\"<expected_output>\", \"/expected_output>\"]\n",
    "\n",
    "css = \"\"\"\n",
    "#red {background-color: #FA9F9D}\n",
    "#amber {background-color: #FFD966}\n",
    "#chatbot-group {\n",
    "    flex-grow: 1;\n",
    "}\n",
    "#chatbot-window {\n",
    "    offsetheight: 100%;\n",
    "}\n",
    "#script-group {\n",
    "    flex-grow: 1;\n",
    "}\n",
    "#chatbot-button {\n",
    "    flex-grow: 1;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "   \n",
    "def execute_fn(state, history, script, exp_out, model, language, stream):\n",
    "    data = json.dumps(\n",
    "        {\n",
    "            \"script\": script,\n",
    "            \"model_family\": models_list[model][\"model_family\"], \n",
    "            \"model_name\": models_list[model][\"model_name\"], \n",
    "            \"language\": language,\n",
    "            \"expected_output\": exp_out,\n",
    "            \"conv_id\": state,\n",
    "            \"stream\": stream\n",
    "        }\n",
    "    )\n",
    "    response = requests.post(\n",
    "        \"http://\" + controller_url + \"/execute\",\n",
    "        data=data,\n",
    "        stream=stream\n",
    "    ).json()\n",
    "    if response[\"error\"]:\n",
    "        out = gr.Textbox(value=response[\"output\"],label=\"Output: ❌️ EERROR ❌️\", elem_id=\"red\")\n",
    "        history.append([\"The script failed with shown error message\", response[\"generated_text\"]])\n",
    "        script = response[\"script\"]\n",
    "    else:\n",
    "        if response[\"output\"] != exp_out:\n",
    "            out = gr.Textbox(value=response[\"output\"],label=\"Output: ⚠️ WARNING ⚠️ value does not match `Expected Output`\", elem_id=\"amber\", interactive=False, show_copy_button=True, lines=7)\n",
    "        else:\n",
    "            out = gr.Textbox(value=response[\"output\"],label=\"Output:\", elem_id=\"--primary-50\", interactive=False, show_copy_button=True, lines=7)\n",
    "    return [history, out, script] \n",
    "\n",
    "def execute_fn_with_stream(state, history, script, exp_out, model, language, stream):\n",
    "    data = json.dumps(\n",
    "        {\n",
    "            \"script\": script,\n",
    "            \"model_family\": models_list[model][\"model_family\"], \n",
    "            \"model_name\": models_list[model][\"model_name\"], \n",
    "            \"language\": language,\n",
    "            \"expected_output\": exp_out,\n",
    "            \"conv_id\": state,\n",
    "            \"stream\": stream\n",
    "        }\n",
    "    )\n",
    "    response = requests.post(\n",
    "        \"http://\" + controller_url + \"/execute\",\n",
    "        data=data,\n",
    "        stream=stream\n",
    "    )\n",
    "    for chunk in response.iter_lines():\n",
    "        json_obj = json.loads(chunk)\n",
    "        if json_obj[\"error\"]:\n",
    "            out = gr.Textbox(value=json_obj[\"output\"],label=\"Output: ❌️ EERROR ❌️\", elem_id=\"red\")\n",
    "            history.append(\n",
    "                [\n",
    "                    \"The script failed with shown error message\",\n",
    "                    # to avoid Gradio Markdown bug related to <output></output> bug\n",
    "                    (\n",
    "                        json_obj[\"generated_text\"] + \"▌\"\n",
    "                    ).replace(\n",
    "                        out_tag[0], \n",
    "                        ex_out_tag[0]\n",
    "                    ).replace(\n",
    "                        out_tag[1], \n",
    "                        ex_out_tag[1]\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "            script = json_obj[\"script\"]\n",
    "        else:\n",
    "            if json_obj[\"output\"] != exp_out:\n",
    "                out = gr.Textbox(value=json_obj[\"output\"],label=\"Output: ⚠️ WARNING ⚠️ value does not match `Expected Output`\", elem_id=\"amber\", interactive=False, show_copy_button=True, lines=7)\n",
    "            else:\n",
    "                out = gr.Textbox(value=json_obj[\"output\"],label=\"Output:\", elem_id=\"--primary-50\", interactive=False, show_copy_button=True, lines=7)\n",
    "        yield [history, out, script]\n",
    "    history[-1][1] = history[-1][1][:-1]\n",
    "    yield history, out, script\n",
    "\n",
    "def can_exec(script):\n",
    "    if script != \"\":\n",
    "        return gr.Button(value=\"Approve and Execute\", interactive=True)\n",
    "    else:\n",
    "        return gr.Button(value=\"Approve and Execute\", interactive=False)\n",
    "\n",
    "    \n",
    "def change_language(language):\n",
    "    \"\"\"\n",
    "    Code Languages\n",
    "    approved language: [('python', 'markdown', 'json', 'html', 'css', 'javascript', 'typescript', 'yaml', 'dockerfile', 'shell', 'r')]\n",
    "    \"\"\"\n",
    "    return [\"\", []] + [gr.Code(value=\"\", language=l_mapping[language], interactive=False)] + [\"\"] * 2\n",
    "\n",
    "def change_model():\n",
    "    return [\"\", []] + [\"\"] * 3\n",
    "\n",
    "def clear_fn():\n",
    "    return [\"\", [], \"\", \"\"] + [gr.Textbox(value=\"\",label=\"Output\", elem_id=\"--primary-50\", interactive=False, show_copy_button=True, lines=7)]\n",
    "\n",
    "def vote(data: gr.LikeData):\n",
    "    if data.liked:\n",
    "        print(\"You upvoted this response: \" + data.value)\n",
    "    else:\n",
    "        print(\"You downvoted this response: \" + data.value)   \n",
    "\n",
    "def add_text(message, history):\n",
    "    history += [[message, None]]\n",
    "    return [\"\", history]\n",
    "\n",
    "def generate_response_with_stream(state, history, model, language, stream): \n",
    "    \"\"\"\n",
    "    Sample Response - to be deleted\n",
    "    \"\"\"\n",
    "            \n",
    "    data = json.dumps(\n",
    "        {\n",
    "            \"prompt\": history[-1][0],\n",
    "            \"model_family\": models_list[model][\"model_family\"], \n",
    "            \"model_name\": models_list[model][\"model_name\"], \n",
    "            \"language\": language,\n",
    "            \"conv_id\": state,\n",
    "            \"stream\": stream\n",
    "        }\n",
    "    )\n",
    "    response = requests.post(\n",
    "        \"http://\" + controller_url + \"/generate\",\n",
    "        data=data,\n",
    "        stream=stream\n",
    "    )\n",
    "    \n",
    "    for chunk in response.iter_lines():\n",
    "        json_obj = json.loads(chunk)\n",
    "        # to avoid Gradio Markdown bug related to <output></output> bug\n",
    "        history[-1][1] = (\n",
    "            json_obj[\"generated_text\"] + \"▌\"\n",
    "        ).replace(\n",
    "            out_tag[0], \n",
    "            ex_out_tag[0]\n",
    "        ).replace(\n",
    "            out_tag[1], \n",
    "            ex_out_tag[1]\n",
    "        )\n",
    "        yield (\n",
    "            json_obj[\"conv_id\"],\n",
    "            history,\n",
    "            json_obj[\"script\"],\n",
    "            json_obj[\"expected_output\"]\n",
    "        )\n",
    "    history[-1][1] = history[-1][1][:-1]\n",
    "    yield (\n",
    "        json_obj[\"conv_id\"],\n",
    "        history,\n",
    "        json_obj[\"script\"],\n",
    "        json_obj[\"expected_output\"]\n",
    "    )\n",
    "\n",
    "def generate_response(state, history, model, language, stream): \n",
    "    \"\"\"\n",
    "    Sample Response - to be deleted\n",
    "    \"\"\"\n",
    "               \n",
    "    data = json.dumps(\n",
    "        {\n",
    "            \"prompt\": history[-1][0],\n",
    "            \"model_family\": models_list[model][\"model_family\"], \n",
    "            \"model_name\": models_list[model][\"model_name\"], \n",
    "            \"language\": language,\n",
    "            \"conv_id\": state,\n",
    "            \"stream\": stream\n",
    "        }\n",
    "    )\n",
    "    response = requests.post(\n",
    "            \"http://\" + controller_url + \"/generate\",\n",
    "            data=data\n",
    "        )\n",
    "\n",
    "    res = json.loads(response.text)[\"generated_text\"]\n",
    "    state = res[\"conv_id\"]\n",
    "    history[-1][1] = json_obj[\"generated_text\"]\n",
    "    return [state, history, res[\"script\"], res[\"expected_output\"]]\n",
    "\n",
    "def web_ui():\n",
    "    with gr.Blocks(css=css, theme=gr.themes.Base(neutral_hue=\"slate\")) as webUI:\n",
    "        state = gr.State(value=\"\")\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            # Welcome to SynthCodeX 🤖️\n",
    "            ### Use this tool to generate and test code using LLMs\n",
    "            \"\"\")\n",
    "        with gr.Row():#equal_height=True):\n",
    "            with gr.Column(scale=2):\n",
    "                with gr.Group(elem_id=\"chatbot-group\"):\n",
    "                    with gr.Row():\n",
    "                        chatbot = gr.Chatbot(elem_id=\"chatbot-window\")\n",
    "                    with gr.Row():\n",
    "                        textbox = gr.Textbox(show_label=False, placeholder=\"Enter your prompt here and press ENTER\", container=False, scale=16)\n",
    "                        submit = gr.Button(value=\"💬️\", variant=\"primary\", scale=2, elem_id=\"chatbot-button\")\n",
    "                        clear_btn = gr.ClearButton(value=\"🗑️\", scale=1, elem_id=\"chatbot-button\")\n",
    "                    \n",
    "            with gr.Column(scale=1):\n",
    "                with gr.Group(elem_id=\"script-group\"):\n",
    "                    language = gr.Dropdown(languages,label='Langauge Selection', value=languages[0]) # Langauge Selection\n",
    "                    model = gr.Dropdown(models_list.keys(),label=\"Model Selection\", value=list(models_list.keys())[0]) # Model Selection\n",
    "                    script = gr.Code(value=\"\",label=\"Script\", language = l_mapping[languages[0]], interactive=False)\n",
    "                    exp_out = gr.Textbox(value=\"\",label=\"Expected Output\", interactive=False)\n",
    "                    execute = gr.Button(value=\"Approve and Execute\", interactive=False)\n",
    "        out = gr.Textbox(value=\"\",label=\"Output\", interactive=False, show_copy_button=True, lines=7)\n",
    "        with gr.Accordion(label=\"Parameters\", open=False):\n",
    "            streaming = gr.Checkbox(label='Stream chat response', value=True)\n",
    "            timeout = gr.Number(label=\"Timeout\", precision=0, minimum=10, maximum=300, value=10)\n",
    "            \n",
    "        language.change(change_language, [language], [state, chatbot, script, out, exp_out], queue=False)\n",
    "        model.change(change_model, None, [state, chatbot, script, out, exp_out], queue=False)\n",
    "        # script.change(can_exec, script, execute, queue=False)\n",
    "        \n",
    "        submit.click(\n",
    "            add_text, \n",
    "            [textbox, chatbot], \n",
    "            [textbox, chatbot],\n",
    "            queue=False\n",
    "        ).then(\n",
    "            generate_response_with_stream if streaming.value else generate_response, \n",
    "            [state, chatbot, model, language, streaming], \n",
    "            [state, chatbot, script, exp_out],\n",
    "            queue=streaming.value\n",
    "        ).then(\n",
    "            can_exec, \n",
    "            script, \n",
    "            execute, \n",
    "            queue=False\n",
    "        )\n",
    "        \n",
    "        textbox.submit(\n",
    "            add_text, \n",
    "            [textbox, chatbot], \n",
    "            [textbox, chatbot],\n",
    "            queue=False\n",
    "        ).then(\n",
    "            generate_response_with_stream if streaming.value else generate_response, \n",
    "            [state, chatbot, model, language, streaming], \n",
    "            [state, chatbot, script, exp_out],\n",
    "            queue=streaming.value\n",
    "        ).then(\n",
    "            can_exec, \n",
    "            script, \n",
    "            execute, \n",
    "            queue=False\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(clear_fn, None, [state, chatbot, script, exp_out, out], queue=False)\n",
    "        execute.click(\n",
    "            execute_fn_with_stream if streaming.value else execute_fn, \n",
    "            [state, chatbot, script, exp_out, model, language, streaming], \n",
    "            [chatbot, out, script], queue=streaming\n",
    "        )\n",
    "        chatbot.like(vote, None, None, queue=False)\n",
    "    return webUI\n",
    "\n",
    "def get_languages_list(url):\n",
    "    return list(json.loads(requests.get(\"http://\" + url + \"/list_languages\").json()).keys())\n",
    "    \n",
    "if __name__ == \"__main__\":    \n",
    "    languages = get_languages_list(controller_url)\n",
    "    UI = web_ui()\n",
    "    UI.queue(concurrency_count=10, status_update_rate=10, api_open=False).launch(debug=True, max_threads=200)\n",
    "\n",
    "    # create events, when chaning model/language selection, it will clear the chat history, scirpt, expected output and output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "1140c6e0-cdec-4f69-873b-6e14650ba440",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<expected_output></expected_output>'"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"<output></output>\".replace(\"<output>\", \"<expected_output>\").replace(\"</output>\", \"</expected_output>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2ae04438-8085-41e7-8b14-ea82d89d13a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "293"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response_o[-9][1][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c2c2517b-1fb4-4464-99a6-129804c20b3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('6914ab3441f0407d',\n",
       " [['List my SageMaker models',\n",
       "   ' Here is how to list your SageMaker models in Python:\\n\\n```python\\nimport sagemaker\\n\\nsagemaker_session = sagemaker.Session()\\n\\nmodels = sagemaker_session.list_models()\\n\\nfor model in models:\\n    print(model.name)\\n```\\n\\n<output>\\nThis will']],\n",
       " 'import sagemaker\\n\\nsagemaker_session = sagemaker.Session()\\n\\nmodels = sagemaker_session.list_models()\\n\\nfor model in models:\\n    print(model.name)',\n",
       " '')"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_o[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384b5a0f-08a4-4e4a-b636-411014e46dd4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reset Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4e548614-e140-48b0-8ca9-402878cd760f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('6914ab3441f0407d', [['List my SageMaker models', ' Here']], '', '')\n",
      "('6914ab3441f0407d', [['List my SageMaker models', ' Here is how to']], '', '')\n",
      "('6914ab3441f0407d', [['List my SageMaker models', ' Here is how to list your SageMaker models in Python']], '', '')\n",
      "('6914ab3441f0407d', [['List my SageMaker models', ' Here is how to list your SageMaker models in Python:\\n\\n```python\\nimport sagemaker\\n\\nsagemaker']], 'import sagemaker\\n\\nsagemaker', '')\n",
      "('6914ab3441f0407d', [['List my SageMaker models', ' Here is how to list your SageMaker models in Python:\\n\\n```python\\nimport sagemaker\\n\\nsagemaker_session = sagemaker.Session()']], 'import sagemaker\\n\\nsagemaker_session = sagemaker.Session()', '')\n",
      "('6914ab3441f0407d', [['List my SageMaker models', ' Here is how to list your SageMaker models in Python:\\n\\n```python\\nimport sagemaker\\n\\nsagemaker_session = sagemaker.Session()\\n\\nmodels = sagemaker_session.list_models()\\n\\nfor']], 'import sagemaker\\n\\nsagemaker_session = sagemaker.Session()\\n\\nmodels = sagemaker_session.list_models()\\n\\nfor', '')\n",
      "('6914ab3441f0407d', [['List my SageMaker models', ' Here is how to list your SageMaker models in Python:\\n\\n```python\\nimport sagemaker\\n\\nsagemaker_session = sagemaker.Session()\\n\\nmodels = sagemaker_session.list_models()\\n\\nfor model in models:\\n    print(model.name)']], 'import sagemaker\\n\\nsagemaker_session = sagemaker.Session()\\n\\nmodels = sagemaker_session.list_models()\\n\\nfor model in models:\\n    print(model.name)', '')\n",
      "('6914ab3441f0407d', [['List my SageMaker models', ' Here is how to list your SageMaker models in Python:\\n\\n```python\\nimport sagemaker\\n\\nsagemaker_session = sagemaker.Session()\\n\\nmodels = sagemaker_session.list_models()\\n\\nfor model in models:\\n    print(model.name)\\n```\\n\\n<output>\\nThis will']], 'import sagemaker\\n\\nsagemaker_session = sagemaker.Session()\\n\\nmodels = sagemaker_session.list_models()\\n\\nfor model in models:\\n    print(model.name)', '')\n",
      "('6914ab3441f0407d', [['List my SageMaker models', ' Here is how to list your SageMaker models in Python:\\n\\n```python\\nimport sagemaker\\n\\nsagemaker_session = sagemaker.Session()\\n\\nmodels = sagemaker_session.list_models()\\n\\nfor model in models:\\n    print(model.name)\\n```\\n\\n<output>\\nThis will print out the names of all models in your SageMaker account.']], 'import sagemaker\\n\\nsagemaker_session = sagemaker.Session()\\n\\nmodels = sagemaker_session.list_models()\\n\\nfor model in models:\\n    print(model.name)', '')\n",
      "('6914ab3441f0407d', [['List my SageMaker models', ' Here is how to list your SageMaker models in Python:\\n\\n```python\\nimport sagemaker\\n\\nsagemaker_session = sagemaker.Session()\\n\\nmodels = sagemaker_session.list_models()\\n\\nfor model in models:\\n    print(model.name)\\n```\\n\\n<output>\\nThis will print out the names of all models in your SageMaker account.\\n</output>\\n\\nTo list the models, we']], 'import sagemaker\\n\\nsagemaker_session = sagemaker.Session()\\n\\nmodels = sagemaker_session.list_models()\\n\\nfor model in models:\\n    print(model.name)', 'This will print out the names of all models in your SageMaker account.')\n",
      "('6914ab3441f0407d', [['List my SageMaker models', ' Here is how to list your SageMaker models in Python:\\n\\n```python\\nimport sagemaker\\n\\nsagemaker_session = sagemaker.Session()\\n\\nmodels = sagemaker_session.list_models()\\n\\nfor model in models:\\n    print(model.name)\\n```\\n\\n<output>\\nThis will print out the names of all models in your SageMaker account.\\n</output>\\n\\nTo list the models, we create a SageMaker']], 'import sagemaker\\n\\nsagemaker_session = sagemaker.Session()\\n\\nmodels = sagemaker_session.list_models()\\n\\nfor model in models:\\n    print(model.name)', 'This will print out the names of all models in your SageMaker account.')\n",
      "('6914ab3441f0407d', [['List my SageMaker models', ' Here is how to list your SageMaker models in Python:\\n\\n```python\\nimport sagemaker\\n\\nsagemaker_session = sagemaker.Session()\\n\\nmodels = sagemaker_session.list_models()\\n\\nfor model in models:\\n    print(model.name)\\n```\\n\\n<output>\\nThis will print out the names of all models in your SageMaker account.\\n</output>\\n\\nTo list the models, we create a SageMaker session, call the']], 'import sagemaker\\n\\nsagemaker_session = sagemaker.Session()\\n\\nmodels = sagemaker_session.list_models()\\n\\nfor model in models:\\n    print(model.name)', 'This will print out the names of all models in your SageMaker account.')\n",
      "('6914ab3441f0407d', [['List my SageMaker models', ' Here is how to list your SageMaker models in Python:\\n\\n```python\\nimport sagemaker\\n\\nsagemaker_session = sagemaker.Session()\\n\\nmodels = sagemaker_session.list_models()\\n\\nfor model in models:\\n    print(model.name)\\n```\\n\\n<output>\\nThis will print out the names of all models in your SageMaker account.\\n</output>\\n\\nTo list the models, we create a SageMaker session, call the list_models() method on it,']], 'import sagemaker\\n\\nsagemaker_session = sagemaker.Session()\\n\\nmodels = sagemaker_session.list_models()\\n\\nfor model in models:\\n    print(model.name)', 'This will print out the names of all models in your SageMaker account.')\n",
      "('6914ab3441f0407d', [['List my SageMaker models', ' Here is how to list your SageMaker models in Python:\\n\\n```python\\nimport sagemaker\\n\\nsagemaker_session = sagemaker.Session()\\n\\nmodels = sagemaker_session.list_models()\\n\\nfor model in models:\\n    print(model.name)\\n```\\n\\n<output>\\nThis will print out the names of all models in your SageMaker account.\\n</output>\\n\\nTo list the models, we create a SageMaker session, call the list_models() method on it, and then print the name attribute of']], 'import sagemaker\\n\\nsagemaker_session = sagemaker.Session()\\n\\nmodels = sagemaker_session.list_models()\\n\\nfor model in models:\\n    print(model.name)', 'This will print out the names of all models in your SageMaker account.')\n",
      "('6914ab3441f0407d', [['List my SageMaker models', ' Here is how to list your SageMaker models in Python:\\n\\n```python\\nimport sagemaker\\n\\nsagemaker_session = sagemaker.Session()\\n\\nmodels = sagemaker_session.list_models()\\n\\nfor model in models:\\n    print(model.name)\\n```\\n\\n<output>\\nThis will print out the names of all models in your SageMaker account.\\n</output>\\n\\nTo list the models, we create a SageMaker session, call the list_models() method on it, and then print the name attribute of each model returned. Let me know if you need any']], 'import sagemaker\\n\\nsagemaker_session = sagemaker.Session()\\n\\nmodels = sagemaker_session.list_models()\\n\\nfor model in models:\\n    print(model.name)', 'This will print out the names of all models in your SageMaker account.')\n",
      "('6914ab3441f0407d', [['List my SageMaker models', ' Here is how to list your SageMaker models in Python:\\n\\n```python\\nimport sagemaker\\n\\nsagemaker_session = sagemaker.Session()\\n\\nmodels = sagemaker_session.list_models()\\n\\nfor model in models:\\n    print(model.name)\\n```\\n\\n<output>\\nThis will print out the names of all models in your SageMaker account.\\n</output>\\n\\nTo list the models, we create a SageMaker session, call the list_models() method on it, and then print the name attribute of each model returned. Let me know if you need any help accessing or using your Sage']], 'import sagemaker\\n\\nsagemaker_session = sagemaker.Session()\\n\\nmodels = sagemaker_session.list_models()\\n\\nfor model in models:\\n    print(model.name)', 'This will print out the names of all models in your SageMaker account.')\n",
      "('6914ab3441f0407d', [['List my SageMaker models', ' Here is how to list your SageMaker models in Python:\\n\\n```python\\nimport sagemaker\\n\\nsagemaker_session = sagemaker.Session()\\n\\nmodels = sagemaker_session.list_models()\\n\\nfor model in models:\\n    print(model.name)\\n```\\n\\n<output>\\nThis will print out the names of all models in your SageMaker account.\\n</output>\\n\\nTo list the models, we create a SageMaker session, call the list_models() method on it, and then print the name attribute of each model returned. Let me know if you need any help accessing or using your SageMaker models!']], 'import sagemaker\\n\\nsagemaker_session = sagemaker.Session()\\n\\nmodels = sagemaker_session.list_models()\\n\\nfor model in models:\\n    print(model.name)', 'This will print out the names of all models in your SageMaker account.')\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def generate_response_with_stream(state, history, model_family, model_name, language, stream): \n",
    "    \"\"\"\n",
    "    Sample Response - to be deleted\n",
    "    \"\"\"\n",
    "            \n",
    "    data = json.dumps(\n",
    "        {\n",
    "            \"prompt\": history[-1][0],\n",
    "            \"model_family\": model_family, \n",
    "            \"model_name\": model_name, \n",
    "            \"language\": language,\n",
    "            \"conv_id\": state,\n",
    "            \"stream\": stream\n",
    "        }\n",
    "    )\n",
    "    response = requests.post(\n",
    "        \"http://\" + controller_url + \"/generate\",\n",
    "        data=data,\n",
    "        stream=stream\n",
    "        )\n",
    "    \n",
    "    for chunk in response.iter_lines():\n",
    "        # print(chunk)\n",
    "        json_obj = json.loads(chunk)\n",
    "        history[-1][1] = json_obj[\"generated_text\"]\n",
    "        yield (\n",
    "            json_obj[\"conv_id\"],\n",
    "            history,\n",
    "            json_obj[\"script\"],\n",
    "            json_obj[\"expected_output\"]\n",
    "        )\n",
    "\n",
    "for i in generate_response_with_stream(\"\", [[\"List my SageMaker models\", None]], \"bedrock\", \"anthropic.claude-v2\", \"Python\", True):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cdb5cd-b194-4714-b059-24652be3009b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f7cd313-1fe8-4088-9886-cea387797db7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://ip-172-31-73-215.ap-southeast-2.compute.internal:8000/generate'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"http://\" + controller_url + \"/generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2929aec4-28d6-4149-91f0-8ca15a9a2eff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\" \\nconsole.log('Hello world!');\", 'Hello world!')\n"
     ]
    }
   ],
   "source": [
    "def extract_script(text, tag_name):\n",
    "    script = text.split(code_block_symbol + tag_name)[1].split( \"\\n\" + code_block_symbol)[0].lstrip(\"\\n\")\n",
    "    expected_output = \"\"\n",
    "    if output_tags[0] in text and output_tags[1] in text:\n",
    "        expected_output = text.split(output_tags[0])[1].split(\"\\n\" + output_tags[1])[0].lstrip(\"\\n\")\n",
    "    return (script, expected_output)\n",
    "\n",
    "\n",
    "text = \"\"\"```js \n",
    "console.log('Hello world!');\n",
    "```\n",
    "<output>\n",
    "Hello world!\n",
    "</output>\"\"\"\n",
    "\n",
    "tag_name = \"js\"\n",
    "code_block_symbol = \"```\"\n",
    "output_tags = [\"<output>\", \"</output>\"]\n",
    "print(extract_script(text, tag_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "28823398-4078-44b9-a1ab-213a08fba320",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 22\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m StreamingResponse(\n\u001b[1;32m     17\u001b[0m         res, \n\u001b[1;32m     18\u001b[0m         media_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/x-ndjson\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m     )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 22\u001b[0m     \u001b[43muvicorn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0.0.0.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m8002\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minfo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/uvicorn/main.py:587\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(app, host, port, uds, fd, loop, http, ws, ws_max_size, ws_max_queue, ws_ping_interval, ws_ping_timeout, ws_per_message_deflate, lifespan, interface, reload, reload_dirs, reload_includes, reload_excludes, reload_delay, workers, env_file, log_config, log_level, access_log, proxy_headers, server_header, date_header, forwarded_allow_ips, root_path, limit_concurrency, backlog, limit_max_requests, timeout_keep_alive, timeout_graceful_shutdown, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_version, ssl_cert_reqs, ssl_ca_certs, ssl_ciphers, headers, use_colors, app_dir, factory, h11_max_incomplete_event_size)\u001b[0m\n\u001b[1;32m    585\u001b[0m     Multiprocess(config, target\u001b[38;5;241m=\u001b[39mserver\u001b[38;5;241m.\u001b[39mrun, sockets\u001b[38;5;241m=\u001b[39m[sock])\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 587\u001b[0m     \u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39muds \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(config\u001b[38;5;241m.\u001b[39muds):\n\u001b[1;32m    589\u001b[0m     os\u001b[38;5;241m.\u001b[39mremove(config\u001b[38;5;241m.\u001b[39muds)  \u001b[38;5;66;03m# pragma: py-win32\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/uvicorn/server.py:61\u001b[0m, in \u001b[0;36mServer.run\u001b[0;34m(self, sockets)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, sockets: Optional[List[socket\u001b[38;5;241m.\u001b[39msocket]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39msetup_event_loop()\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43msockets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msockets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/asyncio/runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m coroutines\u001b[38;5;241m.\u001b[39miscoroutine(main):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma coroutine was expected, got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(main))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "from fastapi.responses import StreamingResponse\n",
    "from fastapi import FastAPI\n",
    "import time\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "def stream():\n",
    "    for i in range(20):\n",
    "        time.sleep(1)\n",
    "        yield [str(i)] * 3\n",
    "        \n",
    "@app.get(\"/generate\")\n",
    "async def generate():\n",
    "    res = stream()\n",
    "    return StreamingResponse(\n",
    "        res, \n",
    "        media_type=\"application/x-ndjson\"\n",
    "    )\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(\n",
    "        app, \n",
    "        host=\"0.0.0.0\", \n",
    "        port=\"8002\",\n",
    "        log_level=\"info\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "08800b48-d39f-4ae5-8c4c-2b4219989e44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "base                     /opt/conda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "da86ad67-85c2-428d-9bf5-79cbc9251221",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"0\", \"0\", \"0\"]\n",
      "[\"1\", \"1\", \"1\"]\n",
      "[\"2\", \"2\", \"2\"]\n",
      "[\"3\", \"3\", \"3\"]\n",
      "[\"4\", \"4\", \"4\"]\n",
      "[\"5\", \"5\", \"5\"]\n",
      "[\"6\", \"6\", \"6\"]\n",
      "[\"7\", \"7\", \"7\"]\n",
      "[\"8\", \"8\", \"8\"]\n",
      "[\"9\", \"9\", \"9\"]\n",
      "[\"10\", \"10\", \"10\"]\n",
      "[\"11\", \"11\", \"11\"]\n",
      "[\"12\", \"12\", \"12\"]\n",
      "[\"13\", \"13\", \"13\"]\n",
      "[\"14\", \"14\", \"14\"]\n",
      "[\"15\", \"15\", \"15\"]\n",
      "[\"16\", \"16\", \"16\"]\n",
      "[\"17\", \"17\", \"17\"]\n",
      "[\"18\", \"18\", \"18\"]\n",
      "[\"19\", \"19\", \"19\"]\n"
     ]
    }
   ],
   "source": [
    "!curl 0.0.0.0:8002/generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "bb35f670-41e9-48cd-a291-8b6523a99d7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "streaming_payload = [\n",
    "    [[\"How to break chatbot with markdown\", \"How to break chatbot with markdown? How to break chatbot with markdown? Simply\"]],\n",
    "    [[\"How to break chatbot with markdown\", \"How to break chatbot with markdown? How to break chatbot with markdown?  Simply put\"]],\n",
    "    [[\"How to break chatbot with markdown\", \"How to break chatbot with markdown? How to break chatbot with markdown?  Simply put some\"]],\n",
    "    [[\"How to break chatbot with markdown\", \"How to break chatbot with markdown? How to break chatbot with markdown?  Simply put some markdown\"]],\n",
    "    [[\"How to break chatbot with markdown\", \"How to break chatbot with markdown? How to break chatbot with markdown?  Simply put some markdown elements\"]],\n",
    "    [[\"How to break chatbot with markdown\", \"How to break chatbot with markdown? How to break chatbot with markdown?  Simply put some markdown elements\\n```python\"]],\n",
    "    [[\"How to break chatbot with markdown\", \"How to break chatbot with markdown? How to break chatbot with markdown?  Simply put some markdown elements\\n```python\\nprint(\\\"Hello World!\\\"```\\n\"]],\n",
    "    [[\"How to break chatbot with markdown\", \"How to break chatbot with markdown? How to break chatbot with markdown?  Simply put some markdown elements\\n```python\\nprint(\\\"Hello World!\\\"```\\n<exp_output>\"]],\n",
    "    [[\"How to break chatbot with markdown\", \"How to break chatbot with markdown? How to break chatbot with markdown?  Simply put some markdown elements\\n```python\\nprint(\\\"Hello World!\\\")\\n```\\n<exp_output>\\nHello World!\"]],\n",
    "    [[\"How to break chatbot with markdown\",  \"How to break chatbot with markdown? How to break chatbot with markdown?  Simply put some markdown elements\\n```python\\nprint(\\\"Hello World!\\\")\\n```\\n<exp_output>\\nHello World!\\n</exp_output>\\n\"]],\n",
    "    [[\"How to break chatbot with markdown\", \"How to break chatbot with markdown? How to break chatbot with markdown?  Simply put some markdown elements\\n```python\\nprint(\\\"Hello World!\\\")\\n```\\n<exp_output>\\nHello World!\\n</exp_output>\\nHopefully\"]],\n",
    "    [[\"How to break chatbot with markdown\", \"How to break chatbot with markdown? How to break chatbot with markdown?  Simply put some markdown elements\\n```python\\nprint(\\\"Hello World!\\\")\\n```\\n<exp_output>\\nHello World!\\n</exp_output>\\nHopefully that\"]],\n",
    "    [[\"How to break chatbot with markdown\", \"How to break chatbot with markdown? How to break chatbot with markdown?  Simply put some markdown elements\\n```python\\nprint(\\\"Hello World!\\\")\\n```\\n<exp_output>\\nHello World!\\n</exp_output>\\nHopefully that was\"]],\n",
    "    [[\"How to break chatbot with markdown\", \"How to break chatbot with markdown? How to break chatbot with markdown?  Simply put some markdown elements\\n```python\\nprint(\\\"Hello World!\\\")\\n```\\n<exp_output>\\nHello World!\\n</exp_output>\\nHopefully that was enough\"]],\n",
    "    [[\"How to break chatbot with markdown\", \"How to break chatbot with markdown? How to break chatbot with markdown?  Simply put some markdown elements\\n```python\\nprint(\\\"Hello World!\\\")\\n```\\n<exp_output>\\nHello World!\\n</exp_output>\\nHopefully that was enough to raise\"]],\n",
    "    [[\"How to break chatbot with markdown\", \"How to break chatbot with markdown? How to break chatbot with markdown?  Simply put some markdown elements\\n```python\\nprint(\\\"Hello World!\\\")\\n```\\n<exp_output>\\nHello World!\\n</exp_output>\\nHopefully that was enough to raise a bug\"]],\n",
    "    [[\"How to break chatbot with markdown\", \"How to break chatbot with markdown? How to break chatbot with markdown?  Simply put some markdown elements\\n```python\\nprint(\\\"Hello World!\\\")\\n```\\n<exp_output>\\nHello World!\\n</exp_output>\\nHopefully that was enough to raise a bug report\"]]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "a32983e8-7298-427f-9c37-782bff70727e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n",
      "Sagemaker notebooks may require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Running on public URL: https://1c1dfde18899bca231.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://1c1dfde18899bca231.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7863 <> https://1c1dfde18899bca231.gradio.live\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import time \n",
    "\n",
    "def add_text(message, history, history1):\n",
    "    history += [[message, None]]\n",
    "    history1 += [[message, None]]\n",
    "    return [\"\", history, history1]\n",
    "\n",
    "def generate_response_with_stream(history, history1):\n",
    "    for payload in streaming_payload:\n",
    "        time.sleep(0.3)\n",
    "        yield [payload, payload]\n",
    "\n",
    "def web_ui():\n",
    "    with gr.Blocks(css=css, theme=gr.themes.Base(neutral_hue=\"slate\")) as webUI:\n",
    "        with gr.Row():\n",
    "            chatbot = gr.Chatbot(render_markdown=False) # Having markdown enabled seems to break streaming\n",
    "            chatbot1 = gr.Chatbot()\n",
    "        with gr.Row():\n",
    "            textbox = gr.Textbox(show_label=False, placeholder=\"Enter your prompt here and press ENTER\", container=False, scale=16)\n",
    "            submit = gr.Button(value=\"💬️\", variant=\"primary\", scale=2)\n",
    "        \n",
    "        submit.click(\n",
    "            add_text, \n",
    "            [textbox, chatbot, chatbot1], \n",
    "            [textbox, chatbot, chatbot1],\n",
    "            queue=False\n",
    "        ).then(\n",
    "            generate_response_with_stream, \n",
    "            [chatbot, chatbot1], \n",
    "            [chatbot, chatbot1],\n",
    "            queue=True\n",
    "        )\n",
    "        \n",
    "        textbox.submit(\n",
    "            add_text, \n",
    "            [textbox, chatbot, chatbot1], \n",
    "            [textbox, chatbot, chatbot1],\n",
    "            queue=False\n",
    "        ).then(\n",
    "            generate_response_with_stream, \n",
    "            [chatbot, chatbot1], \n",
    "            [chatbot, chatbot1],\n",
    "            queue=True\n",
    "        )\n",
    "    return webUI\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    UI = web_ui()\n",
    "    UI.queue(concurrency_count=10, status_update_rate=10, api_open=False).launch(debug=True, max_threads=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ae4a92b5-4e74-48a5-a59a-d9fe51025d8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(streaming_payload[-1][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4551d62-7d37-46d2-8a7c-fcff2d87cb24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-2:452832661640:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "lcc_arn": "arn:aws:sagemaker:ap-southeast-2:734840029783:studio-lifecycle-config/sdocker-kg"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
